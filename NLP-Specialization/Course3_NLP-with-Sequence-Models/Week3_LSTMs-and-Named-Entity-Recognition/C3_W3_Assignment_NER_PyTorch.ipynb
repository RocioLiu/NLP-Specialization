{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Named Entity Recognition (NER)\n",
    "\n",
    "Welcome to the third programming assignment of Course 3. In this assignment, you will learn to build more complicated models with Trax. By completing this assignment, you will be able to: \n",
    "\n",
    "- Design the architecture of a neural network, train it, and test it. \n",
    "- Process features and represents them\n",
    "- Understand word padding\n",
    "- Implement LSTMs\n",
    "- Test with your own sentence\n",
    "\n",
    "## Outline\n",
    "- [Introduction](#0)\n",
    "- [Part 1:  Exploring the data](#1)\n",
    "    - [1.1  Importing the Data](#1.1)\n",
    "    - [1.2  Data generator](#1.2)\n",
    "\t\t- [Exercise 01](#ex01)\n",
    "- [Part 2:  Building the model](#2)\n",
    "\t- [Exercise 02](#ex02)\n",
    "- [Part 3:  Train the Model ](#3)\n",
    "\t- [Exercise 03](#ex03)\n",
    "- [Part 4:  Compute Accuracy](#4)\n",
    "\t- [Exercise 04](#ex04)\n",
    "- [Part 5:  Testing with your own sentence](#5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"0\"></a>\n",
    "# Introduction\n",
    "\n",
    "We first start by defining named entity recognition (NER). NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. \n",
    "\n",
    "For example:\n",
    "\n",
    "<img src = 'ner.png' width=\"width\" height=\"height\" style=\"width:600px;height:150px;\"/>\n",
    "\n",
    "Is labeled as follows: \n",
    "\n",
    "- French: geopolitical entity\n",
    "- Morocco: geographic entity \n",
    "- Christmas: time indicator\n",
    "\n",
    "Everything else that is labeled with an `O` is not considered to be a named entity. In this assignment, you will train a named entity recognition system that could be trained in a few seconds (on a GPU) and will get around 75% accuracy. Then, you will load in the exact version of your model, which was trained for a longer period of time. You could then evaluate the trained version of your model to get 96% accuracy! Finally, you will be able to test your named entity recognition system with your own sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 0, 33], dtype=uint32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip -q install trax==1.3.1\n",
    "\n",
    "import trax \n",
    "from trax import layers as tl\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from utils import get_params, get_vocab\n",
    "import random as rnd\n",
    "\n",
    "# set random seeds to make this notebook easier to replicate\n",
    "trax.supervised.trainer_lib.init_random_number_generators(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#from utils import get_params, get_vocab\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# Part 1:  Exploring the data\n",
    "\n",
    "We will be using a dataset from Kaggle, which we will preprocess for you. The original data consists of four columns, the sentence number, the word, the part of speech of the word, and the tags.  A few tags you might expect to see are: \n",
    "\n",
    "* geo: geographical entity\n",
    "* org: organization\n",
    "* per: person \n",
    "* gpe: geopolitical entity\n",
    "* tim: time indicator\n",
    "* art: artifact\n",
    "* eve: event\n",
    "* nat: natural phenomenon\n",
    "* O: filler word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "\n",
      "SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "\n",
      "ORIGINAL DATA:\n",
      "     Sentence #           Word  POS Tag\n",
      "0  Sentence: 1      Thousands  NNS   O\n",
      "1          NaN             of   IN   O\n",
      "2          NaN  demonstrators  NNS   O\n",
      "3          NaN           have  VBP   O\n",
      "4          NaN        marched  VBN   O\n"
     ]
    }
   ],
   "source": [
    "# display original kaggle data\n",
    "data = pd.read_csv(\"ner_dataset.csv\", encoding = \"ISO-8859-1\") \n",
    "train_sents = open('data/small/train/sentences.txt', 'r').readline()\n",
    "train_labels = open('data/small/train/labels.txt', 'r').readline()\n",
    "print('SENTENCE:', train_sents)\n",
    "print('SENTENCE LABEL:', train_labels)\n",
    "print('ORIGINAL DATA:\\n', data.head(5))\n",
    "#del(data, train_sents, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1048565</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>impact</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048566</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048567</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>Indian</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048568</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>forces</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048569</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence #       Word  POS    Tag\n",
       "1048565  Sentence: 47958     impact   NN      O\n",
       "1048566  Sentence: 47958          .    .      O\n",
       "1048567  Sentence: 47959     Indian   JJ  B-gpe\n",
       "1048568  Sentence: 47959     forces  NNS      O\n",
       "1048569  Sentence: 47959       said  VBD      O\n",
       "1048570  Sentence: 47959       they  PRP      O\n",
       "1048571  Sentence: 47959  responded  VBD      O\n",
       "1048572  Sentence: 47959         to   TO      O\n",
       "1048573  Sentence: 47959        the   DT      O\n",
       "1048574  Sentence: 47959     attack   NN      O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"ner_dataset.csv\", encoding = \"ISO-8859-1\").fillna(method='ffill')\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1.1\"></a>\n",
    "## 1.1  Importing the Data\n",
    "\n",
    "In this part, we will import the preprocessed data and explore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def get_vocab(vocab_path, tags_path):\n",
    "    vocab = {}\n",
    "    with open(vocab_path) as f:\n",
    "        for i, l in enumerate(f.read().splitlines()):\n",
    "            vocab[l] = i  # to avoid the 0\n",
    "        # loading tags (we require this to map tags to their indices)\n",
    "    vocab['<PAD>'] = len(vocab) # 35180\n",
    "    tag_map = {}\n",
    "    with open(tags_path) as f:\n",
    "        for i, t in enumerate(f.read().splitlines()):\n",
    "            tag_map[t] = i \n",
    "    \n",
    "    return vocab, tag_map\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "def get_params(vocab, tag_map, sentences_file, labels_file):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(sentences_file) as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            # replace each token by its index if it is in vocab\n",
    "            # else use index of UNK_WORD\n",
    "            s = [vocab[token] if token in vocab \n",
    "                 else vocab['UNK']\n",
    "                 for token in sentence.split(' ')]\n",
    "            sentences.append(s)\n",
    "\n",
    "    with open(labels_file) as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            # replace each label by its index\n",
    "            l = [tag_map[label] for label in sentence.split(' ')] # I added plus 1 here\n",
    "            labels.append(l) \n",
    "    return sentences, labels, len(sentences)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding the words**  \n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network.\n",
    "\n",
    ">Build a dictionary that maps words to integers. Later we're going to pad our input vectors with zeros, so make sure the integers **start at 1, not 0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a dictionary that maps words to integers\n",
    "words_count = Counter(data[\"Word\"])\n",
    "vocab_to_int = {w: (i+1) for i, w in enumerate(words_count)}\n",
    "vocab_to_int['UNK'] = len(vocab_to_int)\n",
    "#vocab_to_int['<PAD>'] = len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Thousands': 114,\n",
       " 'of': 26354,\n",
       " 'demonstrators': 110,\n",
       " 'have': 5485,\n",
       " 'marched': 65,\n",
       " 'through': 515,\n",
       " 'London': 261,\n",
       " 'to': 23213,\n",
       " 'protest': 237,\n",
       " 'the': 52573}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(words_count.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Thousands': 1,\n",
       "  'of': 2,\n",
       "  'demonstrators': 3,\n",
       "  'have': 4,\n",
       "  'marched': 5,\n",
       "  'through': 6,\n",
       "  'London': 7,\n",
       "  'to': 8,\n",
       "  'protest': 9,\n",
       "  'the': 10},\n",
       " {'hardliner': 35175,\n",
       "  'indicative': 35176,\n",
       "  '3700': 35177,\n",
       "  'Bermel': 35178,\n",
       "  'UNK': 35178})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(vocab_to_int.items())[:10]), dict(list(vocab_to_int.items())[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  35179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stats about vocabulary\n",
    "print('Unique words: ', len(vocab_to_int))  # should ~ 74000+\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 1,\n",
       " 'B-per': 2,\n",
       " 'I-gpe': 3,\n",
       " 'B-geo': 4,\n",
       " 'B-tim': 5,\n",
       " 'I-tim': 6,\n",
       " 'B-art': 7,\n",
       " 'I-nat': 8,\n",
       " 'I-org': 9,\n",
       " 'I-per': 10,\n",
       " 'B-eve': 11,\n",
       " 'I-eve': 12,\n",
       " 'B-org': 13,\n",
       " 'I-geo': 14,\n",
       " 'B-nat': 15,\n",
       " 'B-gpe': 16,\n",
       " 'I-art': 17}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build a dictionary that maps tags to integers\n",
    "tag_to_int = {t: (i+1) for i, t in enumerate(set(data[\"Tag\"]))}\n",
    "tag_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sentences to integers and store the sentences in a new list called `sents_ints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params2(vocab_to_int, tag_to_int, df):\n",
    "    sents_int = []\n",
    "    labels_int = [] \n",
    "    \n",
    "    sents_w = df.groupby('Sentence #')['Word'].apply(list).values\n",
    "    for sent in sents_w:\n",
    "        sent_int = [vocab_to_int[w] if w in vocab_to_int else vocab_to_int['UNK'] for w in sent]\n",
    "        sents_int.append(sent_int)\n",
    "    \n",
    "    sents_tags = df.groupby('Sentence #')['Tag'].apply(list).values\n",
    "    for sent_tags in sents_tags:\n",
    "        label_int = [tag_to_int[t] for t in sent_tags]\n",
    "        labels_int.append(label_int)\n",
    "    \n",
    "    return sents_int, labels_int, len(sents_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_int, labels_int, sent_size = get_params2(vocab_to_int, tag_to_int, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train-val-test split 7:1.5:1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac1, split_frac2 = 0.7, 0.15\n",
    "\n",
    "## split data into training, validation, and test data (sentences, labels, and len)\n",
    "split_idx = int(len(sents_int)*split_frac1)\n",
    "train_sent, train_label = sents_int[:split_idx], labels_int[:split_idx]\n",
    "\n",
    "split_idx2 = split_idx + int(len(sents_int)*split_frac2)\n",
    "valid_sent, valid_label = sents_int[split_idx:split_idx2], labels_int[split_idx:split_idx2]\n",
    "test_sent, test_label = sents_int[split_idx2:], labels_int[split_idx2:]\n",
    "\n",
    "train_len, valid_len, test_len = len(train_sent), len(valid_sent), len(test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33571, 7193, 7195)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len, valid_len, test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 10,\n",
       " 16,\n",
       " 2,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, tag_map = get_vocab('data/large/words.txt', 'data/large/tags.txt')\n",
    "t_sentences, t_labels, t_size = get_params(vocab, tag_map, 'data/large/train/sentences.txt', 'data/large/train/labels.txt')\n",
    "v_sentences, v_labels, v_size = get_params(vocab, tag_map, 'data/large/val/sentences.txt', 'data/large/val/labels.txt')\n",
    "test_sentences, test_labels, test_size = get_params(vocab, tag_map, 'data/large/test/sentences.txt', 'data/large/test/labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33570, 7194, 7194)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_size, v_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47958"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_size + v_size + test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-geo': 1,\n",
       " 'B-gpe': 2,\n",
       " 'B-per': 3,\n",
       " 'I-geo': 4,\n",
       " 'B-org': 5,\n",
       " 'I-org': 6,\n",
       " 'B-tim': 7,\n",
       " 'B-art': 8,\n",
       " 'I-art': 9,\n",
       " 'I-per': 10,\n",
       " 'I-gpe': 11,\n",
       " 'I-tim': 12,\n",
       " 'B-nat': 13,\n",
       " 'B-eve': 14,\n",
       " 'I-eve': 15,\n",
       " 'I-nat': 16}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vocab` is a dictionary that translates a word string to a unique number. Given a sentence, you can represent it as an array of numbers translating with this dictionary. The dictionary contains a `<PAD>` token. \n",
    "\n",
    "When training an LSTM using batches, all your input sentences must be the same size. To accomplish this, you set the length of your sentences to a certain number and add the generic `<PAD>` token to fill all the empty spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab[\"the\"]: 9\n",
      "padded token: 35180\n"
     ]
    }
   ],
   "source": [
    "# vocab translates from a word to a unique number\n",
    "print('vocab[\"the\"]:', vocab[\"the\"])\n",
    "# Pad token\n",
    "print('padded token:', vocab['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab[\"the\"]: 10\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# vocab translates from a word to a unique number\n",
    "print('vocab[\"the\"]:', vocab_to_int[\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tag_map corresponds to one of the possible tags a word can have. Run the cell below to see the possible classes you will be predicting. The prepositions in the tags mean:\n",
    "* I: Token is inside an entity.\n",
    "* B: Token begins an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'B-art': 8, 'I-art': 9, 'I-per': 10, 'I-gpe': 11, 'I-tim': 12, 'B-nat': 13, 'B-eve': 14, 'I-eve': 15, 'I-nat': 16}\n"
     ]
    }
   ],
   "source": [
    "print(tag_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 1, 'B-per': 2, 'I-gpe': 3, 'B-geo': 4, 'B-tim': 5, 'I-tim': 6, 'B-art': 7, 'I-nat': 8, 'I-org': 9, 'I-per': 10, 'B-eve': 11, 'I-eve': 12, 'B-org': 13, 'I-geo': 14, 'B-nat': 15, 'B-gpe': 16, 'I-art': 17}\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "print(tag_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the coding scheme that tags the entities is a minimal one where B- indicates the first token in a multi-token entity, and I- indicates one in the middle of a multi-token entity. If you had the sentence \n",
    "\n",
    "**\"Sharon flew to Miami on Friday\"**\n",
    "\n",
    "the outputs would look like:\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n",
    "\n",
    "your tags would reflect three tokens beginning with B-, since there are no multi-token entities in the sequence. But if you added Sharon's last name to the sentence: \n",
    "\n",
    "**\"Sharon Floyd flew to Miami on Friday\"**\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "Floyd  I-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n",
    "\n",
    "then your tags would change to show first \"Sharon\" as B-per, and \"Floyd\" as I-per, where I- indicates an inner token in a multi-token sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of outputs is tag_map 17\n",
      "Num of vocabulary words: 35181\n",
      "The vocab size is 35181\n",
      "The training size is 33570\n",
      "The validation size is 7194\n",
      "An example of the first sentence is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]\n",
      "An example of its corresponding label is [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Exploring information about the data\n",
    "print('The number of outputs is tag_map', len(tag_map))\n",
    "# The number of vocabulary tokens (including <PAD>)\n",
    "g_vocab_size = len(vocab)\n",
    "print(f\"Num of vocabulary words: {g_vocab_size}\")\n",
    "print('The vocab size is', len(vocab))\n",
    "print('The training size is', t_size)\n",
    "print('The validation size is', v_size)\n",
    "print('An example of the first sentence is', t_sentences[0])\n",
    "print('An example of its corresponding label is', t_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of outputs is tag_map 17\n",
      "The vocab size is 35179\n",
      "The training size is 33571\n",
      "The validation size is 7193\n",
      "The validation size is 7195\n",
      "An example of the first sentence is [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 10, 16, 2, 17, 18, 19, 20, 21, 22]\n",
      "An example of its corresponding label is [1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 16, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Exploring information about the data\n",
    "print('The number of outputs is tag_map', len(tag_to_int))\n",
    "# The number of vocabulary tokens (including <PAD>)\n",
    "g_vocab_size = len(vocab_to_int)\n",
    "#print(f\"Num of vocabulary words: {g_vocab_size}\")\n",
    "print(f'The vocab size is {g_vocab_size}')\n",
    "print(f'The training size is {train_len}')\n",
    "print(f'The validation size is {valid_len}')\n",
    "print(f'The validation size is {test_len}')\n",
    "print(f'An example of the first sentence is {train_sent[0]}')\n",
    "print(f'An example of its corresponding label is {train_label[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see that we have already encoded each sentence into a tensor by converting it into a number. We also have 16 possible classes, as shown in the tag map.\n",
    "\n",
    "\n",
    "<a name=\"1.2\"></a>\n",
    "## 1.2  Data generator\n",
    "\n",
    "In python, a generator is a function that behaves like an iterator. It will return the next item. Here is a [link](https://wiki.python.org/moin/Generators) to review python generators. \n",
    "\n",
    "In many AI applications it is very useful to have a data generator. You will now implement a data generator for our NER application.\n",
    "\n",
    "<a name=\"ex01\"></a>\n",
    "### Exercise 01\n",
    "\n",
    "**Instructions:** Implement a data generator function that takes in `batch_size, x, y, pad, shuffle` where x is a large list of sentences, and y is a list of the tags associated with those sentences and pad is a pad value. Return a subset of those inputs in a tuple of two arrays `(X,Y)`. Each is an array of dimension (`batch_size, max_len`), where `max_len` is the length of the longest sentence *in that batch*. You will pad the X and Y examples with the pad argument. If `shuffle=True`, the data will be traversed in a random form.\n",
    "\n",
    "**Details:**\n",
    "\n",
    "This code as an outer loop  \n",
    "```\n",
    "while True:  \n",
    "...  \n",
    "yield((X,Y))  \n",
    "```\n",
    "\n",
    "Which runs continuously in the fashion of generators, pausing when yielding the next values. We will generate a batch_size output on each pass of this loop.    \n",
    "\n",
    "It has two inner loops. \n",
    "1. The first stores in temporal lists the data samples to be included in the next batch, and finds the maximum length of the sentences contained in it. By adjusting the length to include only the size of the longest sentence in each batch, overall computation is reduced. \n",
    "\n",
    "2. The second loop moves those inputs from the temporal list into NumPy arrays pre-filled with pad values.\n",
    "\n",
    "There are three slightly out of the ordinary features. \n",
    "1. The first is the use of the NumPy `full` function to fill the NumPy arrays with a pad value. See [full function documentation](https://numpy.org/doc/1.18/reference/generated/numpy.full.html).\n",
    "\n",
    "2. The second is tracking the current location in the incoming lists of sentences. Generators variables hold their values between invocations, so we create an `index` variable, initialize to zero, and increment by one for each sample included in a batch. However, we do not use the `index` to access the positions of the list of sentences directly. Instead, we use it to select one index from a list of indexes. In this way, we can change the order in which we traverse our original list, keeping untouched our original list.  \n",
    "\n",
    "3. The third also relates to wrapping. Because `batch_size` and the length of the input lists are not aligned, gathering a batch_size group of inputs may involve wrapping back to the beginning of the input loop. In our approach, it is just enough to reset the `index` to 0. We can re-shuffle the list of indexes to produce different batches each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "class EntityDataset:\n",
    "    def __init__(self, sentences, labels, pad):\n",
    "        # sentences: [[61, 249, 19, ..., 722, 21], [...]]\n",
    "        # labels: [[15, 15, 15, ..., 15, 15], [...]]\n",
    "        # pad - an integer representing a pad character\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.max_len = max([len(i) for i in self.sentences])\n",
    "        self.pad = pad\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        sentence = self.sentences[item] # get only one sentence\n",
    "        labels = self.labels[item]\n",
    "        \n",
    "        ids = []\n",
    "        target_tag = []\n",
    "        \n",
    "        mask = [1] * len(sentence)\n",
    "        token_type_ids = [0] * len(sentence)\n",
    "        \n",
    "        # right pads to the max_len in dataset(or batch) at\n",
    "        # the size of the longest sentence in each batch\n",
    "        padding_len = self.max_len - len(sentence)\n",
    "    \n",
    "        sentence = sentence + ([self.pad] * padding_len)\n",
    "        mask = mask + ([self.pad] * padding_len)\n",
    "        # Define sentence A and B indices associated to 1st and 2nd sentences\n",
    "        # [0, 0, ..., 1,...,1]\n",
    "        token_type_ids = token_type_ids + ([self.pad] * padding_len)\n",
    "        labels = labels + ([self.pad] * padding_len)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(sentence, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(mask, dtype=torch.long),  \n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensor datasets\n",
    "# similar as TensorDataset\n",
    "train_dataset = EntityDataset(\n",
    "    sentences=train_sent,\n",
    "    labels=train_label,\n",
    "    pad=0\n",
    ")\n",
    "\n",
    "valid_dataset = EntityDataset(\n",
    "    sentences=valid_sent,\n",
    "    labels=valid_label,\n",
    "    pad=0\n",
    ")\n",
    "test_dataset = EntityDataset(\n",
    "    sentences=test_sent,\n",
    "    labels=test_label,\n",
    "    pad=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 10, 16,  2,\n",
       "         17, 18, 19, 20, 21, 22,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([ 1,  1,  1,  1,  1,  1,  4,  1,  1,  1,  1,  1,  4,  1,  1,  1,  1,  1,\n",
       "         16,  1,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_workers as a positive integer will turn on multi-process data loading \n",
    "# with the specified number of loader worker processes.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE,\n",
    "                              shuffle=True, num_workers=4)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, \n",
    "                              shuffle=True, num_workers=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE,  \n",
    "                             shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at a batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  346,    46,  1972,  ...,     0,     0,     0],\n",
       "         [  305,   353,     4,  ...,     0,     0,     0],\n",
       "         [17847,    32,    10,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [31480,   569,    10,  ...,     0,     0,     0],\n",
       "         [   13,    79, 10712,  ...,     0,     0,     0],\n",
       "         [   43,  1067,  3480,  ...,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[ 1,  1,  1,  ...,  0,  0,  0],\n",
       "         [16,  1,  1,  ...,  0,  0,  0],\n",
       "         [ 1,  1,  1,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [13,  1,  1,  ...,  0,  0,  0],\n",
       "         [ 4,  1,  1,  ...,  0,  0,  0],\n",
       "         [ 1,  1,  1,  ...,  0,  0,  0]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_batch = next(iter(train_dataloader))\n",
    "one_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  346,    46,  1972,  ...,     0,     0,     0],\n",
      "        [  305,   353,     4,  ...,     0,     0,     0],\n",
      "        [17847,    32,    10,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [31480,   569,    10,  ...,     0,     0,     0],\n",
      "        [   13,    79, 10712,  ...,     0,     0,     0],\n",
      "        [   43,  1067,  3480,  ...,     0,     0,     0]]) \n",
      "\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "print(one_batch['input_ids'], '\\n')\n",
    "print(f\"Batch size: {len(one_batch['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: data_generator\n",
    "def data_generator(batch_size, x, y, pad, shuffle=False, verbose=False):\n",
    "    '''\n",
    "      Input: \n",
    "        batch_size - integer describing the batch size\n",
    "        x - list containing sentences where words are represented as integers\n",
    "        y - list containing tags associated with the sentences\n",
    "        shuffle - Shuffle the data order\n",
    "        pad - an integer representing a pad character\n",
    "        verbose - Print information during runtime\n",
    "      Output:\n",
    "        a tuple containing 2 elements:\n",
    "        X - np.ndarray of dim (batch_size, max_len) of padded sentences\n",
    "        Y - np.ndarray of dim (batch_size, max_len) of tags associated with the sentences in X\n",
    "    '''\n",
    "    \n",
    "    # count the number of lines in data_lines\n",
    "    num_lines = len(x)\n",
    "    \n",
    "    # create an array with the indexes of data_lines that can be shuffled\n",
    "    lines_index = [*range(num_lines)]\n",
    "    \n",
    "    # shuffle the indexes if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(lines_index)\n",
    "    \n",
    "    index = 0 # tracks current location in x, y\n",
    "    while True:\n",
    "        buffer_x = [0] * batch_size # Temporal array to store the raw x data for this batch\n",
    "        buffer_y = [0] * batch_size # Temporal array to store the raw y data for this batch\n",
    "                \n",
    "  ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        \n",
    "        # Copy into the temporal buffers the sentences in x[index : index + batch_size] \n",
    "        # along with their corresponding labels y[index : index + batch_size]\n",
    "        # Find maximum length of sentences in x[index : index + batch_size] for this batch. \n",
    "        # Reset the index if we reach the end of the data set, and shuffle the indexes if needed.\n",
    "        max_len = 0\n",
    "        for i in range(batch_size):\n",
    "             # if the index is greater than or equal to the number of lines in x\n",
    "            if index >= num_lines:\n",
    "                # then reset the index to 0\n",
    "                index = 0\n",
    "                # re-shuffle the indexes if shuffle is set to True\n",
    "                if shuffle:\n",
    "                    rnd.shuffle(lines_index)\n",
    "            \n",
    "            # The current position is obtained using `lines_index[index]`\n",
    "            # Store the x value at the current position into the buffer_x\n",
    "            buffer_x[i] = x[lines_index[index]]\n",
    "            \n",
    "            # Store the y value at the current position into the buffer_y\n",
    "            buffer_y[i] = y[lines_index[index]]\n",
    "            \n",
    "            lenx = len(buffer_x[i])    #length of current x[]\n",
    "            if lenx > max_len:\n",
    "                max_len = lenx         #max_len tracks longest x[]\n",
    "            \n",
    "            # increment index by one\n",
    "            index += 1\n",
    "\n",
    "\n",
    "        # create X,Y, NumPy arrays of size (batch_size, max_len) 'full' of pad value\n",
    "        X = np.full((batch_size, max_len), pad)\n",
    "        Y = np.full((batch_size, max_len), pad)\n",
    "\n",
    "        # copy values from lists to NumPy arrays. Use the buffered values\n",
    "        for i in range(batch_size):\n",
    "            # get the example (sentence as a tensor)\n",
    "            # in `buffer_x` at the `i` index\n",
    "            x_i = buffer_x[i]\n",
    "            \n",
    "            # similarly, get the example's labels\n",
    "            # in `buffer_y` at the `i` index\n",
    "            y_i = buffer_y[i]\n",
    "            \n",
    "            # Walk through each word in x_i\n",
    "            for j in range(len(x_i)):\n",
    "                # store the word in x_i at position j into X\n",
    "                X[i, j] = x_i[j]\n",
    "                \n",
    "                # store the label in y_i at position j into Y\n",
    "                Y[i, j] = y_i[j]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "        if verbose: print(\"index=\", index)\n",
    "        yield((X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index= 5\n",
      "index= 2\n",
      "(5, 30) (5, 30) (5, 30) (5, 30)\n",
      "[    0     1     2     3     4     5     6     7     8     9    10    11\n",
      "    12    13    14     9    15     1    16    17    18    19    20    21\n",
      " 35180 35180 35180 35180 35180 35180] \n",
      " [    0     0     0     0     0     0     1     0     0     0     0     0\n",
      "     1     0     0     0     0     0     2     0     0     0     0     0\n",
      " 35180 35180 35180 35180 35180 35180]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "mini_sentences = t_sentences[0: 8]\n",
    "mini_labels = t_labels[0: 8]\n",
    "dg = data_generator(batch_size, mini_sentences, mini_labels, vocab[\"<PAD>\"], shuffle=False, verbose=True)\n",
    "X1, Y1 = next(dg)\n",
    "X2, Y2 = next(dg)\n",
    "print(Y1.shape, X1.shape, Y2.shape, X2.shape)\n",
    "print(X1[0][:], \"\\n\", Y1[0][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**   \n",
    "```\n",
    "index= 5\n",
    "index= 2\n",
    "(5, 30) (5, 30) (5, 30) (5, 30)\n",
    "[    0     1     2     3     4     5     6     7     8     9    10    11\n",
    "    12    13    14     9    15     1    16    17    18    19    20    21\n",
    " 35180 35180 35180 35180 35180 35180] \n",
    " [    0     0     0     0     0     0     1     0     0     0     0     0\n",
    "     1     0     0     0     0     0     2     0     0     0     0     0\n",
    " 35180 35180 35180 35180 35180 35180]  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 35]) torch.Size([5, 35]) torch.Size([5, 35]) torch.Size([5, 35])\n",
      "tensor([ 596,  173,  323,  117,   69, 1524,   14,   10, 3606, 8063, 3898,   20,\n",
      "        2434,   59,  152, 6187, 1415,   12,  667,   14, 2245,   22,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]) \n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 4, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "BATCH_SIZE = 5\n",
    "mini_sentences = train_sent[0: 8]\n",
    "mini_labels = train_label[0: 8]\n",
    "mini_dataset = EntityDataset(mini_sentences, mini_labels, pad=0)\n",
    "mini_dataloader = DataLoader(mini_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "batch1 = next(iter(mini_dataloader))\n",
    "batch2 = next(iter(mini_dataloader))\n",
    "print(batch1['input_ids'].shape, batch1['labels'].shape, batch2['input_ids'].shape, batch2['labels'].shape)\n",
    "#print(Y1.shape, X1.shape, Y2.shape, X2.shape\n",
    "print(batch1['input_ids'][0], \"\\n\", batch1['labels'][0])\n",
    "#print(X1[0][:], \"\\n\", Y1[0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "# Part 2:  Building the model\n",
    "\n",
    "You will now implement the model. You will be using Google's TensorFlow. Your model will be able to distinguish the following:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "<img src = 'ner1.png' width=\"width\" height=\"height\" style=\"width:500px;height:150px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The model architecture will be as follows: \n",
    "\n",
    "<img src = 'ner2.png' width=\"width\" height=\"height\" style=\"width:600px;height:250px;\"/>\n",
    "\n",
    "Concretely: \n",
    "\n",
    "* Use the input tensors you built in your data generator\n",
    "* Feed it into an Embedding layer, to produce more semantic entries\n",
    "* Feed it into an LSTM layer\n",
    "* Run the output through a linear layer\n",
    "* Run the result through a log softmax layer to get the predicted class for each word.\n",
    "\n",
    "Good news! We won't make you implement the LSTM unit drawn above. However, we will ask you to build the model. \n",
    "\n",
    "<a name=\"ex02\"></a>\n",
    "### Exercise 02\n",
    "\n",
    "**Instructions:** Implement the initialization step and the forward function of your Named Entity Recognition system.  \n",
    "Please utilize help function e.g. `help(tl.Dense)` for more information on a layer\n",
    "   \n",
    "- [tl.Serial](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/combinators.py#L26): Combinator that applies layers serially (by function composition).\n",
    "    - You can pass in the layers as arguments to `Serial`, separated by commas. \n",
    "    - For example: `tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))` \n",
    "\n",
    "\n",
    "-  [tl.Embedding](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113): Initializes the embedding. In this case it is the dimension of the model by the size of the vocabulary. \n",
    "    - `tl.Embedding(vocab_size, d_feature)`.\n",
    "    - `vocab_size` is the number of unique words in the given vocabulary.\n",
    "    - `d_feature` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).\n",
    "    \n",
    "\n",
    "-  [tl.LSTM](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/rnn.py#L87):`Trax` LSTM layer of size d_model. \n",
    "    - `LSTM(n_units)` Builds an LSTM layer of n_cells.\n",
    "\n",
    "\n",
    "\n",
    "-  [tl.Dense](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L28):  A dense layer.\n",
    "    - `tl.Dense(n_units)`: The parameter `n_units` is the number of units chosen for this dense layer.  \n",
    "\n",
    "\n",
    "- [tl.LogSoftmax](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L242): Log of the output probabilities.\n",
    "    - Here, you don't need to set any parameters for `LogSoftMax()`.\n",
    " \n",
    "\n",
    "**Online documentation**\n",
    "\n",
    "- [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators)\n",
    "\n",
    "- [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)\n",
    "\n",
    "-  [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM)\n",
    "\n",
    "-  [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)\n",
    "\n",
    "- [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax)\n",
    "\n",
    "\n",
    "- [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "\n",
    "- [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)\n",
    "\n",
    "- [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
    "\n",
    "- [nn.functional.log_softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html)\n",
    "\n",
    "- [nn.LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-geo': 1,\n",
       " 'B-gpe': 2,\n",
       " 'B-per': 3,\n",
       " 'I-geo': 4,\n",
       " 'B-org': 5,\n",
       " 'I-org': 6,\n",
       " 'B-tim': 7,\n",
       " 'B-art': 8,\n",
       " 'I-art': 9,\n",
       " 'I-per': 10,\n",
       " 'I-gpe': 11,\n",
       " 'I-tim': 12,\n",
       " 'B-nat': 13,\n",
       " 'B-eve': 14,\n",
       " 'I-eve': 15,\n",
       " 'I-nat': 16}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 1,\n",
       " 'B-per': 2,\n",
       " 'I-gpe': 3,\n",
       " 'B-geo': 4,\n",
       " 'B-tim': 5,\n",
       " 'I-tim': 6,\n",
       " 'B-art': 7,\n",
       " 'I-nat': 8,\n",
       " 'I-org': 9,\n",
       " 'I-per': 10,\n",
       " 'B-eve': 11,\n",
       " 'I-eve': 12,\n",
       " 'B-org': 13,\n",
       " 'I-geo': 14,\n",
       " 'B-nat': 15,\n",
       " 'B-gpe': 16,\n",
       " 'I-art': 17}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some references:\n",
    "* [What does `next(self.parameters()).data` mean?](https://discuss.pytorch.org/t/what-does-next-self-parameters-data-mean/1458/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER(nn.Module):\n",
    "    \"\"\"\n",
    "      Input: \n",
    "        vocab_size - integer containing the size of the vocabulary\n",
    "        d_model - integer describing the embedding size (it's equal to hidden_dim here)\n",
    "      Output:\n",
    "        model - a torch serial model\n",
    "    \"\"\"    \n",
    "    def __init__(self, vocab_size, d_model, tags=tag_to_int):\n",
    "        super(NER, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.tags = tags\n",
    "    \n",
    "        # embedding layer  \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        #self.lstm = nn.LSTM(vocab_size, d_model, batch_first=True)\n",
    "        self.lstm = nn.LSTM(d_model, d_model, batch_first=True)\n",
    "        self.fc = nn.Linear(d_model, len(self.tags))\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(x, hidden) # the hidden should be initiate by below method\n",
    "        out = F.log_softmax(self.fc(lstm_out), axis=1)\n",
    "        \n",
    "        return out, hidden\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data # weight.size() is [len(vocab_size), d_model] here\n",
    "        \n",
    "        # weight.new() creates a tensor that has the same data type, same device as the \n",
    "        # produced parameter.\n",
    "        hidden = (weight.new(batch_size, self.n_hidden).zero_(),\n",
    "                  weight.new(batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER(\n",
      "  (embedding): Embedding(35180, 50)\n",
      "  (lstm): LSTM(50, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=17, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ parameters\n",
    "vocab_size = len(vocab_to_int) + 1 # +1 for the 0 padding\n",
    "d_model = 50 # equals to hidden_dim\n",
    "tags = tag_to_int\n",
    "\n",
    "model = NER(vocab_size, d_model, tag_to_int)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Size([200, 50])  \n",
    "200 = 4 * 50, but why  it'll multiply 50 with 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight \n",
      " torch.Size([35180, 50]) \n",
      " tensor([[-1.3859,  0.9134,  0.4161,  ..., -0.4522, -0.0485,  1.3399],\n",
      "        [ 0.3594,  0.4091, -0.1342,  ...,  0.1172, -1.6185, -1.3530],\n",
      "        [-1.1494, -0.1436,  0.3586,  ...,  2.1872, -0.2721,  0.1462],\n",
      "        ...,\n",
      "        [ 1.1929,  0.4636, -1.6177,  ...,  0.6422,  0.7722, -1.3244],\n",
      "        [-0.9102, -1.8794,  0.6376,  ...,  0.1213, -0.1119,  0.7838],\n",
      "        [ 0.6361, -1.1705,  0.4815,  ...,  0.5103,  0.3008,  1.9215]]) \n",
      "\n",
      "\n",
      "lstm.weight_ih_l0 \n",
      " torch.Size([200, 50]) \n",
      " tensor([[-0.0705, -0.1408, -0.1270,  ..., -0.0405,  0.1412, -0.1313],\n",
      "        [ 0.0531, -0.0198,  0.1404,  ..., -0.0072,  0.0681, -0.0247],\n",
      "        [-0.0289, -0.0465, -0.0084,  ...,  0.0682,  0.0267,  0.1083],\n",
      "        ...,\n",
      "        [ 0.0504,  0.1206,  0.0336,  ...,  0.1367,  0.0569,  0.1367],\n",
      "        [ 0.0008, -0.0679,  0.1190,  ...,  0.0418, -0.0813, -0.0383],\n",
      "        [-0.1100, -0.0296,  0.1373,  ..., -0.0153,  0.0898,  0.0014]]) \n",
      "\n",
      "\n",
      "lstm.weight_hh_l0 \n",
      " torch.Size([200, 50]) \n",
      " tensor([[-0.0183,  0.0762,  0.0471,  ..., -0.0602, -0.0746, -0.0403],\n",
      "        [-0.0704, -0.0541,  0.0973,  ..., -0.0687,  0.0656, -0.0920],\n",
      "        [ 0.1278, -0.0116,  0.1006,  ..., -0.0704,  0.1032, -0.0513],\n",
      "        ...,\n",
      "        [-0.0793,  0.1366,  0.0492,  ...,  0.0535, -0.0161,  0.0761],\n",
      "        [ 0.1266,  0.1163, -0.0933,  ..., -0.0760,  0.0692,  0.1375],\n",
      "        [ 0.1345,  0.1120, -0.0926,  ..., -0.0026, -0.0663,  0.1075]]) \n",
      "\n",
      "\n",
      "lstm.bias_ih_l0 \n",
      " torch.Size([200]) \n",
      " tensor([ 0.1123,  0.1191, -0.0237, -0.0575,  0.0389,  0.0044, -0.0382, -0.0038,\n",
      "         0.1090,  0.0460,  0.0036, -0.0717,  0.0706, -0.1333,  0.0294,  0.1053,\n",
      "        -0.1304, -0.0023, -0.0750,  0.0559,  0.0442, -0.1105,  0.0399, -0.0577,\n",
      "         0.1348,  0.1056, -0.0655,  0.0897,  0.1249, -0.0070,  0.1223,  0.0754,\n",
      "         0.0049, -0.0160,  0.0733,  0.0721,  0.1106, -0.0928, -0.0563,  0.1243,\n",
      "         0.0063, -0.0058, -0.1052, -0.0117, -0.0527,  0.0932,  0.0521,  0.1315,\n",
      "        -0.0941,  0.0183,  0.0508, -0.0665,  0.1288,  0.1103,  0.0483, -0.1404,\n",
      "         0.0875,  0.0463,  0.0694,  0.0492,  0.0575, -0.1094, -0.1300, -0.0037,\n",
      "         0.0544, -0.0289, -0.0554, -0.1143,  0.0021,  0.0049,  0.0538, -0.0853,\n",
      "        -0.0482, -0.0114, -0.1269,  0.1318,  0.1388,  0.0718,  0.0396,  0.0592,\n",
      "        -0.0684,  0.0561,  0.0526,  0.1131,  0.0094, -0.1382,  0.0962, -0.0674,\n",
      "        -0.0758, -0.0612, -0.0081,  0.1410,  0.0486,  0.1198,  0.1099, -0.0375,\n",
      "        -0.0663, -0.1144, -0.1274,  0.1169, -0.0829,  0.1240,  0.0143, -0.0964,\n",
      "         0.0850,  0.1061, -0.1266,  0.0751,  0.0336, -0.0035, -0.0894,  0.1248,\n",
      "         0.0008,  0.0503, -0.0580,  0.0451,  0.0629, -0.0225,  0.1241, -0.0724,\n",
      "        -0.0530, -0.1402, -0.1249, -0.0572, -0.0047,  0.1394,  0.1365, -0.0504,\n",
      "        -0.0836, -0.0049,  0.0662,  0.0743, -0.0158, -0.0002,  0.0984,  0.0948,\n",
      "         0.0075, -0.0309,  0.0704, -0.0971,  0.0793,  0.0399, -0.1295,  0.0725,\n",
      "         0.1150,  0.0056,  0.0934, -0.0024,  0.1078, -0.1356, -0.0876,  0.0428,\n",
      "         0.0206, -0.0356,  0.0260, -0.1183,  0.1208, -0.0364,  0.0723,  0.1226,\n",
      "         0.1180, -0.1162, -0.0483,  0.0830, -0.0120, -0.0908,  0.1264, -0.1305,\n",
      "         0.1161,  0.0743,  0.0853,  0.0669,  0.0947, -0.0886,  0.0555,  0.1131,\n",
      "         0.0756, -0.0809, -0.0187,  0.0263,  0.0347,  0.0730,  0.0989,  0.1211,\n",
      "         0.0474, -0.0915, -0.0996,  0.0707,  0.1298, -0.1106, -0.0570, -0.0127,\n",
      "        -0.0758, -0.1226, -0.1042,  0.0137,  0.0555, -0.0204,  0.0634, -0.0130]) \n",
      "\n",
      "\n",
      "lstm.bias_hh_l0 \n",
      " torch.Size([200]) \n",
      " tensor([ 0.1171, -0.1105, -0.0706,  0.0920, -0.0499,  0.0782,  0.1051, -0.0718,\n",
      "        -0.0364,  0.1132,  0.1381,  0.0408,  0.1130, -0.1261, -0.0563,  0.0198,\n",
      "         0.0840,  0.0793,  0.0181,  0.1167, -0.0650,  0.0977, -0.1218, -0.0739,\n",
      "         0.0911, -0.1261,  0.0902,  0.0578,  0.0400,  0.0228, -0.1365,  0.1157,\n",
      "         0.0583, -0.0798, -0.0311,  0.0186, -0.0263,  0.1076,  0.0440, -0.1370,\n",
      "        -0.0456,  0.0030, -0.1172,  0.0720,  0.0484, -0.0814, -0.1035, -0.0216,\n",
      "         0.0850, -0.0838,  0.0995, -0.0640, -0.0149,  0.0691, -0.1279, -0.0612,\n",
      "        -0.0341, -0.0548, -0.1085, -0.0851,  0.1239,  0.0726,  0.0788, -0.0379,\n",
      "        -0.1131,  0.1177,  0.1326,  0.0054,  0.0087, -0.0986, -0.1027,  0.1206,\n",
      "        -0.0465,  0.0969,  0.0144,  0.0295,  0.1399, -0.1176,  0.0982, -0.0026,\n",
      "        -0.0729, -0.1272,  0.0098,  0.0251,  0.0769,  0.0790, -0.0616, -0.0261,\n",
      "         0.0605,  0.0736,  0.0007, -0.1002,  0.0302, -0.0388, -0.1123, -0.0348,\n",
      "        -0.0258,  0.0103, -0.0296,  0.0669,  0.0062,  0.0677, -0.0799,  0.1361,\n",
      "        -0.0369,  0.0504,  0.1368, -0.0586,  0.0304, -0.0281,  0.0643, -0.0486,\n",
      "         0.0920, -0.0540,  0.1299,  0.1094,  0.0145,  0.0058,  0.0020,  0.0757,\n",
      "        -0.0375, -0.0988,  0.0087,  0.1060,  0.0415, -0.1354,  0.0015, -0.0513,\n",
      "         0.0804,  0.1383,  0.0409, -0.0440, -0.0259,  0.0150, -0.0583, -0.1059,\n",
      "        -0.1223,  0.0461,  0.1123, -0.1317,  0.0621,  0.0572,  0.0821,  0.0865,\n",
      "        -0.1408,  0.0140,  0.1311,  0.1193,  0.1054, -0.0914,  0.0812, -0.1260,\n",
      "         0.1097, -0.0135,  0.0581, -0.1078,  0.0370,  0.1192, -0.0734, -0.1054,\n",
      "        -0.0633, -0.0363, -0.0893, -0.0056,  0.0233, -0.1298, -0.0464,  0.0615,\n",
      "        -0.0971,  0.1333, -0.0181,  0.0952, -0.0851,  0.1171, -0.0289,  0.0527,\n",
      "        -0.0135,  0.1023, -0.0639, -0.0941,  0.0840,  0.1281,  0.1065, -0.0113,\n",
      "         0.1028,  0.0428,  0.1239, -0.1067, -0.0276,  0.0110, -0.0837,  0.0085,\n",
      "        -0.1014,  0.0637, -0.1028, -0.1232,  0.1219, -0.0372,  0.0167, -0.0036]) \n",
      "\n",
      "\n",
      "fc.weight \n",
      " torch.Size([17, 50]) \n",
      " tensor([[-1.3942e-01, -8.6028e-02, -4.9635e-02, -7.9930e-02,  8.3624e-02,\n",
      "         -4.4040e-02,  9.2692e-02, -1.2623e-01, -1.4037e-01,  9.2070e-02,\n",
      "          7.0200e-02,  8.6856e-02, -8.1741e-02,  7.2889e-02,  8.5053e-02,\n",
      "         -6.9525e-02,  4.4034e-02,  8.7383e-02,  7.3226e-03, -9.7825e-03,\n",
      "         -3.5662e-02, -9.4680e-02, -8.4230e-02,  1.0584e-01, -7.8487e-02,\n",
      "         -1.2101e-01,  1.2691e-02, -1.5545e-02, -7.1609e-03, -7.7370e-02,\n",
      "         -8.3881e-02,  2.2834e-02,  3.1778e-02,  1.1528e-01, -7.5917e-02,\n",
      "         -1.3737e-01,  3.5299e-02, -1.1088e-01, -3.3809e-02,  4.7598e-03,\n",
      "          4.3399e-02, -1.2030e-01,  2.8417e-02, -5.2504e-02,  2.2156e-03,\n",
      "         -1.2674e-01,  1.1757e-01,  9.9922e-03, -1.2230e-01,  1.0775e-01],\n",
      "        [ 1.0872e-02, -3.2051e-02, -5.8379e-02,  5.2151e-02,  3.4749e-03,\n",
      "          4.9379e-02, -1.1051e-01,  1.5182e-02,  1.1704e-01, -3.0059e-02,\n",
      "         -9.6861e-02, -9.4354e-02, -7.7305e-02,  4.2817e-02,  7.4716e-02,\n",
      "          1.2792e-01,  2.0483e-02,  1.2786e-01, -2.1251e-02, -1.1607e-01,\n",
      "         -2.3684e-02, -1.1524e-01, -4.5092e-02, -3.8248e-02,  1.5353e-02,\n",
      "         -7.0990e-02,  4.6530e-02, -1.0166e-01,  9.2052e-02,  7.2850e-03,\n",
      "         -1.0443e-01, -1.4042e-01, -1.3835e-02, -3.2099e-02,  6.0862e-02,\n",
      "          8.6818e-03, -4.6731e-02, -2.9855e-02, -4.3392e-02,  1.6295e-02,\n",
      "          1.3660e-01,  5.9275e-02, -5.1942e-02,  2.4633e-02,  6.5329e-02,\n",
      "          6.2311e-02,  3.7145e-02,  1.9670e-02, -7.5747e-02,  1.2886e-01],\n",
      "        [ 1.5169e-03, -3.4257e-02,  9.0224e-02, -2.0262e-02, -9.2451e-02,\n",
      "         -1.4031e-01, -1.2931e-01,  1.1706e-01, -3.7905e-02, -7.0323e-02,\n",
      "          1.1172e-01,  8.7815e-02,  1.1869e-01,  1.3586e-01,  8.3348e-04,\n",
      "          1.2325e-01, -1.1237e-01,  1.0974e-01,  4.5961e-02,  1.0675e-01,\n",
      "          1.2982e-01, -4.9441e-02, -6.0506e-02, -1.0385e-01, -8.1386e-02,\n",
      "          1.1632e-01, -1.2574e-01,  4.1254e-02,  1.2973e-01, -3.3245e-02,\n",
      "          1.2906e-01, -5.6810e-02,  7.6609e-02,  1.2025e-01,  8.3739e-03,\n",
      "         -1.1822e-03,  1.5094e-02,  1.1619e-01,  1.2610e-01, -2.0232e-02,\n",
      "          9.8527e-02, -1.2802e-01, -1.3540e-01,  2.8744e-02,  6.7987e-02,\n",
      "          3.6507e-02,  1.0253e-01, -3.6600e-02,  3.1561e-03,  7.4872e-02],\n",
      "        [-7.3390e-02,  1.2615e-01,  1.2934e-01,  7.0872e-02, -2.2329e-02,\n",
      "         -6.9110e-02,  9.9781e-02, -5.8952e-02,  3.1229e-02, -1.1893e-01,\n",
      "          5.6398e-02, -5.2650e-02,  1.0533e-01, -9.0593e-02, -1.2624e-01,\n",
      "          1.0712e-01, -9.7946e-02,  5.6054e-02, -1.2294e-01, -9.3731e-02,\n",
      "          1.3219e-01, -2.9194e-02,  1.2666e-01, -9.1462e-02, -2.4116e-02,\n",
      "         -7.0371e-02,  1.1646e-01, -1.0945e-01,  3.8020e-02,  7.0619e-02,\n",
      "          7.8641e-02, -8.7007e-02,  1.5631e-02, -9.5394e-02,  1.2309e-01,\n",
      "         -1.0861e-01,  1.1884e-01,  1.1623e-01, -9.2177e-03, -3.0562e-02,\n",
      "         -4.9963e-02, -1.3588e-01, -7.2318e-02,  4.0453e-02, -1.3980e-01,\n",
      "          1.4049e-01,  7.5615e-02, -1.0363e-01, -1.3430e-01,  5.4694e-02],\n",
      "        [ 3.5018e-02,  1.0154e-01,  7.8426e-02,  1.2644e-01, -9.0103e-02,\n",
      "          7.4146e-02,  8.4688e-02, -4.5903e-02, -9.8785e-02, -4.5041e-02,\n",
      "          9.0271e-02, -6.3161e-02, -8.2197e-02, -7.3979e-02, -4.3903e-02,\n",
      "         -5.4659e-02,  4.7354e-02,  3.0915e-02,  9.3818e-02, -5.4178e-02,\n",
      "          1.0107e-01,  9.7950e-04, -1.1891e-01, -5.1752e-02, -1.5794e-02,\n",
      "         -1.7553e-02, -5.2795e-02, -1.1977e-01, -6.2308e-02,  5.2497e-02,\n",
      "         -1.0674e-01,  9.0365e-02,  1.1274e-01,  1.4096e-01,  1.3699e-02,\n",
      "         -1.5274e-03, -3.9452e-02,  1.1231e-01,  1.2357e-01,  1.0927e-01,\n",
      "         -5.8853e-02, -6.3544e-02, -1.3156e-01, -6.3327e-02, -5.4006e-02,\n",
      "         -4.4881e-02,  1.3304e-02, -5.3894e-02,  6.1057e-02,  5.0007e-02],\n",
      "        [-9.1710e-02, -1.0748e-01,  1.7917e-02, -1.0869e-01, -4.0921e-02,\n",
      "         -1.3137e-01, -7.1788e-02, -9.0371e-02,  1.0332e-01, -1.8589e-02,\n",
      "         -8.6592e-02, -2.3197e-02, -1.3610e-02,  2.6614e-02, -1.0539e-01,\n",
      "         -8.7819e-03,  4.1937e-02,  1.1393e-01, -3.8245e-02,  6.6606e-02,\n",
      "         -9.0366e-03, -6.3847e-02, -6.7457e-02,  3.6131e-02,  4.6432e-02,\n",
      "          6.8637e-02, -7.4773e-02,  1.0119e-01, -4.1340e-02, -7.6059e-02,\n",
      "          3.0112e-02,  1.3811e-01,  1.2838e-01,  9.4128e-02, -8.7934e-02,\n",
      "          7.4430e-03, -6.1355e-02,  8.5293e-02, -2.5648e-02, -1.4018e-01,\n",
      "          7.8005e-02,  1.9801e-02,  8.2077e-02,  1.2340e-01,  8.3222e-02,\n",
      "         -3.0741e-02, -1.4205e-03,  2.7461e-02,  3.3063e-02, -8.2130e-02],\n",
      "        [ 1.3563e-01,  2.9128e-02,  1.0355e-01, -1.4282e-02, -1.0600e-01,\n",
      "          1.3544e-01,  8.1159e-02, -1.0481e-01, -9.6790e-02, -1.0628e-01,\n",
      "         -8.7400e-02,  5.6955e-02, -1.3963e-01,  3.5428e-02,  1.3406e-01,\n",
      "          9.7733e-03,  1.3785e-01, -1.4516e-02,  3.4775e-02,  1.2770e-02,\n",
      "          4.5239e-02, -1.2919e-01, -4.8412e-02,  1.2921e-01, -8.9086e-02,\n",
      "          3.8403e-02, -1.0786e-01,  8.1589e-02,  6.1676e-02,  2.1390e-03,\n",
      "         -9.7459e-02, -6.7514e-02, -1.0690e-01,  5.7067e-02,  7.4829e-02,\n",
      "          3.4223e-02, -5.2612e-02,  8.1037e-02, -4.0905e-02,  1.8225e-02,\n",
      "          7.9539e-02,  4.4673e-02, -5.4897e-02,  1.3940e-01, -1.1687e-01,\n",
      "          6.2384e-02, -8.9453e-02, -9.3167e-03, -8.3870e-02,  5.1504e-03],\n",
      "        [ 8.2922e-02, -1.0641e-01,  9.4947e-03,  6.4406e-02,  5.7459e-02,\n",
      "         -7.6949e-02,  7.9762e-02, -1.2181e-02, -7.9589e-02,  9.0015e-02,\n",
      "          8.4499e-02,  7.0564e-02, -5.3207e-02, -6.8425e-02,  5.9792e-02,\n",
      "          8.9681e-02, -6.8422e-02,  4.9520e-02,  2.3773e-02, -5.4396e-02,\n",
      "          4.3273e-02, -1.0369e-01,  6.7849e-02, -1.1224e-02, -6.2540e-02,\n",
      "         -1.3263e-01, -1.1585e-01,  9.6475e-02, -1.0126e-01,  2.9171e-02,\n",
      "          2.2041e-02,  9.0013e-02,  1.1358e-01, -9.5215e-02,  5.5151e-02,\n",
      "          9.4507e-02, -7.2265e-02,  8.7197e-02,  3.6238e-02,  6.2146e-02,\n",
      "          1.8059e-02, -8.3405e-02,  1.3578e-01, -1.4135e-01, -1.6278e-02,\n",
      "         -1.0074e-01,  1.0970e-01, -2.1896e-02, -1.1459e-01, -1.1593e-01],\n",
      "        [-7.9117e-02,  9.8412e-02,  1.9093e-02, -1.2618e-01, -1.9516e-02,\n",
      "          3.8922e-02,  8.6108e-02,  5.0708e-02,  5.8345e-02, -6.8965e-02,\n",
      "         -4.5466e-02, -1.1701e-01,  5.1421e-02,  1.0195e-01, -3.1641e-02,\n",
      "         -1.2496e-01, -9.9284e-02,  1.3138e-01,  7.3635e-02,  3.9937e-02,\n",
      "         -1.3760e-01, -1.2469e-01,  7.8736e-02,  2.3805e-02, -6.7021e-02,\n",
      "          8.4075e-03, -1.2014e-01, -9.4818e-02, -6.4693e-02,  9.8346e-02,\n",
      "         -1.0737e-01,  1.1364e-01,  8.3121e-02, -2.2816e-02, -2.5902e-02,\n",
      "         -1.1471e-02, -5.9431e-03,  4.6851e-02, -4.3113e-02,  7.3749e-02,\n",
      "          1.1558e-01,  1.0499e-01, -6.8146e-02,  3.2549e-02, -1.3485e-01,\n",
      "          8.1427e-02, -1.0224e-02,  8.7011e-02, -9.8436e-02, -1.4430e-02],\n",
      "        [ 1.1903e-01, -5.7637e-02, -9.1491e-02,  8.9939e-02, -6.0081e-02,\n",
      "          5.3543e-02, -9.6625e-02, -1.1230e-01,  3.7513e-02, -9.3406e-02,\n",
      "         -2.8206e-02,  1.4657e-02, -6.8186e-03,  8.6162e-02, -3.5760e-02,\n",
      "          8.1417e-02, -7.3061e-02, -8.2368e-02,  1.3630e-01, -5.3694e-02,\n",
      "          4.0947e-02,  7.8385e-02,  1.0385e-04,  1.1667e-01,  6.0440e-02,\n",
      "          1.1996e-02, -2.7419e-03,  2.9844e-02,  1.3456e-01, -1.1432e-01,\n",
      "         -1.9483e-02, -1.2138e-01,  1.0719e-01,  4.8741e-03,  6.5309e-03,\n",
      "          1.0265e-01,  1.1326e-01,  6.3461e-02, -7.5904e-02,  1.3490e-01,\n",
      "          2.1631e-02, -1.3586e-01,  4.3942e-02, -3.9358e-02,  6.6737e-02,\n",
      "          1.1367e-02, -2.9549e-02, -1.4571e-02,  3.3617e-03,  2.3751e-02],\n",
      "        [-9.0139e-03, -4.2799e-02,  8.4471e-02,  1.5902e-02,  4.6938e-02,\n",
      "         -6.0519e-02, -1.2799e-01, -9.2270e-03, -8.6881e-02,  9.7993e-02,\n",
      "          8.5253e-03,  3.6875e-02,  5.1044e-02, -6.6656e-03,  4.7354e-02,\n",
      "         -4.7194e-02, -9.0659e-03, -8.6793e-02, -1.2496e-01, -1.3622e-01,\n",
      "          4.2411e-03, -9.8271e-02,  4.7939e-03, -9.0493e-02,  8.9640e-03,\n",
      "         -1.8728e-02,  5.4520e-02, -1.3044e-02,  7.7723e-02, -1.3132e-01,\n",
      "         -2.9749e-02,  9.5872e-02,  1.1028e-01,  1.9086e-03, -5.9505e-02,\n",
      "         -1.8743e-02,  6.7220e-02,  6.4229e-02,  1.3639e-01, -5.9820e-02,\n",
      "         -5.2430e-02, -4.5693e-02, -2.0031e-02, -9.7680e-02,  6.9544e-02,\n",
      "          1.0182e-01,  8.9474e-02, -1.3668e-01,  9.9042e-02,  1.3829e-01],\n",
      "        [ 6.3981e-02,  3.7679e-02,  4.0131e-02, -9.0406e-02,  1.1444e-01,\n",
      "         -4.5087e-03, -1.4513e-02,  8.9633e-03,  3.2084e-02,  5.1511e-02,\n",
      "          1.1623e-01,  1.2712e-01, -7.7088e-02, -3.3721e-03, -1.0386e-01,\n",
      "         -6.3879e-02, -1.1569e-01,  3.1683e-02,  1.3558e-01,  5.3835e-02,\n",
      "          9.6160e-03,  2.0061e-02,  8.5466e-03, -6.1870e-02, -8.2050e-02,\n",
      "          1.2700e-01, -3.0017e-02,  2.9302e-02,  1.6964e-02,  7.5263e-02,\n",
      "         -1.0862e-01,  9.6264e-02,  2.0909e-02,  1.3870e-01, -2.1349e-02,\n",
      "          5.0076e-02,  4.9927e-02, -5.6275e-02, -9.6154e-02, -1.0302e-01,\n",
      "         -2.7025e-03, -4.5454e-02, -2.8350e-02,  8.3168e-02, -3.5598e-02,\n",
      "          1.0896e-01, -1.2014e-01,  2.4337e-02,  8.3584e-02, -1.8430e-02],\n",
      "        [ 1.4472e-02, -4.6570e-03,  1.1353e-01, -9.8772e-02,  1.0511e-01,\n",
      "         -1.0415e-01,  6.5521e-02, -1.0827e-02,  5.7493e-02, -1.1666e-01,\n",
      "         -9.9101e-03, -5.2069e-02,  2.8831e-02,  8.9545e-02,  1.1502e-01,\n",
      "          2.5385e-02, -1.1328e-01, -1.0227e-01, -9.5345e-02, -1.1080e-01,\n",
      "         -1.2756e-01, -1.3634e-01, -9.7984e-02,  6.7703e-02,  1.1716e-01,\n",
      "          3.7980e-02, -1.1161e-01,  9.7047e-03, -1.2400e-01, -1.3232e-01,\n",
      "         -7.7075e-04, -8.3377e-02,  3.1864e-02, -1.3260e-01, -2.9128e-02,\n",
      "          5.5266e-02, -8.0259e-02, -1.2972e-01,  9.4517e-02,  1.3329e-01,\n",
      "          5.6380e-02, -3.8785e-02, -9.2321e-02, -1.0486e-01, -2.3803e-02,\n",
      "         -1.3636e-01,  3.5382e-02, -1.2926e-01, -1.0701e-02,  1.2944e-01],\n",
      "        [-3.7871e-02,  1.4904e-02,  1.3998e-02,  1.1698e-02, -1.0570e-01,\n",
      "          8.8219e-02,  1.0383e-01, -9.2766e-02, -6.8922e-02,  5.9906e-02,\n",
      "         -7.2468e-02, -2.8991e-02,  3.4409e-02,  7.0554e-03, -5.2219e-03,\n",
      "          3.7068e-02, -3.6008e-02,  8.4614e-02,  1.2012e-01,  1.2484e-01,\n",
      "         -7.2827e-02,  5.6137e-02, -5.2443e-02,  1.2821e-01, -1.3336e-01,\n",
      "          3.1970e-02,  6.4343e-02,  6.9110e-02, -2.1566e-02, -4.9236e-02,\n",
      "          6.7038e-03,  9.7525e-02, -3.6847e-02,  6.4926e-03,  8.2943e-02,\n",
      "          5.9617e-02,  3.8604e-04, -7.0940e-02,  2.9243e-02, -1.0688e-01,\n",
      "         -3.6164e-02,  1.1827e-01, -1.1356e-01, -4.3344e-02,  7.4105e-02,\n",
      "          1.2007e-01, -5.5565e-02, -9.4498e-02, -1.7486e-02,  1.1033e-02],\n",
      "        [ 1.1039e-01, -7.3451e-02, -1.3921e-01, -3.6684e-02,  8.8840e-02,\n",
      "         -1.1153e-01,  1.3982e-02,  1.1056e-01, -5.9892e-02, -6.4621e-02,\n",
      "          1.3630e-02,  5.2946e-02, -5.6388e-02,  4.6768e-02,  8.8138e-02,\n",
      "         -7.6223e-02, -2.4921e-02, -1.2615e-01,  1.3490e-01,  8.5351e-02,\n",
      "          1.2389e-01,  3.8302e-02, -7.2407e-02,  8.3971e-02, -1.2100e-01,\n",
      "         -3.0881e-03, -6.8428e-02,  4.4528e-02,  1.9785e-02, -8.8281e-02,\n",
      "          3.1371e-02,  1.3162e-01,  1.0245e-01, -3.8988e-02,  4.5037e-02,\n",
      "          3.7896e-02, -1.1809e-01, -1.0511e-01, -7.4119e-02,  5.9437e-03,\n",
      "          8.3421e-03, -1.0570e-02,  7.0622e-02, -1.3808e-01, -2.0555e-02,\n",
      "          1.6344e-02, -5.7579e-02,  6.7864e-02, -7.9179e-02, -6.2968e-02],\n",
      "        [ 7.6896e-02, -7.2309e-02, -9.6238e-02, -1.9608e-02,  4.3096e-02,\n",
      "         -4.0287e-02,  3.3010e-02,  1.3872e-01,  1.1586e-01,  2.0032e-02,\n",
      "         -2.7631e-02,  4.2287e-02,  3.0676e-02, -1.1999e-01, -1.3181e-03,\n",
      "         -1.2635e-01, -1.1954e-01, -7.6526e-02,  1.0173e-01, -9.2417e-02,\n",
      "          3.2130e-02,  5.8479e-02, -5.0997e-02, -7.8138e-02, -1.1685e-01,\n",
      "          3.3307e-02,  1.1049e-01,  7.6185e-03,  5.2895e-02,  3.6710e-02,\n",
      "          1.0539e-01,  1.3952e-01, -1.3046e-01,  2.0491e-02, -1.2968e-01,\n",
      "          1.0683e-01, -4.5938e-02, -9.1431e-02, -9.2326e-03, -5.8563e-02,\n",
      "         -4.3620e-04, -1.2984e-02, -3.8528e-02, -1.2840e-01,  7.9602e-02,\n",
      "         -1.1909e-01,  1.2679e-01, -1.3247e-02,  2.4252e-02,  3.9820e-02],\n",
      "        [ 1.0105e-01,  6.4048e-02, -1.2738e-01,  8.9100e-02,  1.1978e-01,\n",
      "          1.3638e-01, -4.7602e-02,  1.0318e-01,  1.2050e-01,  7.1263e-02,\n",
      "         -7.9507e-02,  1.2791e-01,  9.9062e-02, -1.3979e-01,  1.0519e-01,\n",
      "         -9.0058e-02,  9.9189e-02, -7.7382e-02,  1.2951e-01, -7.7071e-02,\n",
      "         -1.0051e-01,  1.0285e-01,  1.1623e-01,  1.8626e-02,  3.5985e-02,\n",
      "          1.1868e-01, -1.0495e-01,  7.9232e-03, -1.3048e-01, -6.0394e-02,\n",
      "          2.9753e-02,  1.1043e-01, -5.7516e-02, -3.9214e-02,  6.4816e-02,\n",
      "         -4.7765e-02,  3.6132e-02,  2.4310e-02, -1.0316e-01, -6.3378e-02,\n",
      "          1.0923e-02,  7.5897e-02, -1.0645e-02,  3.0149e-02, -6.6315e-02,\n",
      "         -1.4671e-02,  6.6526e-02, -1.3295e-01, -1.0607e-01, -6.3214e-02]]) \n",
      "\n",
      "\n",
      "fc.bias \n",
      " torch.Size([17]) \n",
      " tensor([-0.0613, -0.0679, -0.0078,  0.1042,  0.1025,  0.1296, -0.0734, -0.0039,\n",
      "         0.1140, -0.1275,  0.0001,  0.1007,  0.1019, -0.0952,  0.0745, -0.0289,\n",
      "        -0.0958]) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, '\\n', param.size(), '\\n', param.data, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "# Part 3:  Train the Model \n",
    "\n",
    "This section will train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "rnd.seed(33)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create training data, mask pad id=35180 for training.\n",
    "train_generator = trax.supervised.inputs.add_loss_weights(\n",
    "    data_generator(batch_size, t_sentences, t_labels, vocab['<PAD>'], True),\n",
    "    id_to_mask=vocab['<PAD>'])\n",
    "\n",
    "# Create validation data, mask pad id=35180 for training.\n",
    "eval_generator = trax.supervised.inputs.add_loss_weights(\n",
    "    data_generator(batch_size, v_sentences, v_labels, vocab['<PAD>'], True),\n",
    "    id_to_mask=vocab['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object add_loss_weights at 0x7f9d005be318>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: train_model\n",
    "def train_model(NER, train_generator, eval_generator, train_steps=1, output_dir='model'):\n",
    "    '''\n",
    "    Input: \n",
    "        NER - the model you are building\n",
    "        train_generator - The data generator for training examples\n",
    "        eval_generator - The data generator for validation examples,\n",
    "        train_steps - number of training steps\n",
    "        output_dir - folder to save your model\n",
    "    Output:\n",
    "        training_loop - a trax supervised training Loop\n",
    "    '''\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    train_task = training.TrainTask(\n",
    "      train_generator, # A train data generator\n",
    "      loss_layer = tl.CrossEntropyLoss(), # A cross-entropy loss function\n",
    "      optimizer = trax.optimizers.Adam(0.01),  # The adam optimizer\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "      labeled_data = eval_generator, # A labeled data generator\n",
    "      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], # Evaluate with cross-entropy loss and accuracy\n",
    "      n_eval_batches = 10 # Number of batches to use on each evaluation\n",
    "    )\n",
    "\n",
    "    training_loop = training.Loop(\n",
    "        NER, # A model to train\n",
    "        train_task, # A train task\n",
    "        eval_task = eval_task, # The evaluation task\n",
    "        output_dir = output_dir) # The output directory\n",
    "\n",
    "    # Train with train_steps\n",
    "    training_loop.run(n_steps = train_steps)\n",
    "    ### END CODE HERE ###\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to GPU, if available\n",
    "model.to(device)\n",
    "\n",
    "for e in trange(range(1, epochs+1)):\n",
    "    mdoel.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
